% TODO: Extract to settings
\chapter{An ADER-DG scheme for the Navier-Stokes Equations}\label{chap:methods}
This chapter describes the numerical and physical background that is needed to simulate reacting fluids with high accuracy.
We start with a description of the arbitrary high order derivatives (\ader{}) discontinuous Galerkin (\dg{}) method, with a focus on equations that contain both advective and diffusive terms.
We then seque into a short description of the \muscl{} scheme, which is a second order finite volume method.

We conclude this chapter with a description of finite volume limiting and adaptive mesh refinement (\amr{}).
The limiting combines both numerical methods to achieve a stable, high-order method and the \amr{} allows us to perform high quality simulations with a smaller computational cost.
\section{Reactive Compressible Navier Stokes Equations}\label{sec:navier-stokes}
\newcommand{\diffCoeff}{\varepsilon}
\newcommand{\hyperFluxDef}{
  \begin{pmatrix}
    \Qj \\
    \Qv  \otimes \Qj + \bm{I} \pressure  \\
    \Qv \cdot (\bm{I} \QE + \bm{I} \pressure) \\
    \Qj \QZZ
  \end{pmatrix}
}
\newcommand{\viscFluxDef}{
  \begin{pmatrix}
    0\\
     \stressT (\Q, \gradQ)  \\
     \Qv \cdot \stressT (\Q, \gradQ) - \kappa \gradient{T}\\
     0\\
   \end{pmatrix}
}

Fluid motion can be described by the compressible Navier Stokes equations.
We follow the description in~\cite{dumbser2010arbitrary} for the Navier Stokes part.
The coupling with the advection-diffusion-reaction equation follows~\cite{hidalgo2011ader} which describes this equation set for the one-dimensional case.
In the following, we will denote the spatial coordinates as $\bm{x} = \left( x,z \right)$ and $\bm{x} = \left( x,y,z \right)$ for the two and three-dimensional case respectively.
The variable $t$ corresponds to the current physical time.
We solve the \pde{} in the conservative form% \pcref{eq:conservation-law-gradient}
\begin{equation}
 \label{eq:equation-set} 
%  \begin{array}{l}
%  \text{mass cons.} \\
%  \text{momentum cons.} \\
%  \text{energy cons.} \\
%  \text{cont.\ gas} 
% \end{array}
% :
\quad
  \pdv{}{t}
  \underbrace{
  \begin{pmatrix}
    \Qrho\\
    \Qj\\
    \QE\\
    \QZ
    \end{pmatrix}}_{\Q}
  +
  \divergence{
  \underbrace{
  \left(
   \underbrace{\hyperFluxDef}_{\hyperFlux(\Q)}
+
\underbrace{\viscFluxDef}_{\viscFlux(\Q, \gradQ)}
  \right)}_{\flux(\Q, \gradQ)}}
 =
  \underbrace{
  \begin{pmatrix}
    \source[\Qrho\phantom{\Qrho}]\\
    \source[\Qj]\\
    \source[\QE]\\
    \source[\QZ]
    \end{pmatrix}}_{\source(\Q, \bm{x}, t)}
\end{equation}
which describes the time evolution of variables $\Q$ with respect to a flux $\flux(\Q, \gradQ)$ and a source $\source(\Q)$.
The vector of conserved quantities is given by
\begin{equation}
  \label{eq:conserved-variables}
 \Q = \left( \Qrho, \Qj, \QE, \QZ \right),
\end{equation}
where $\Qrho$ is the density, $\Qj$ is the two or three-dimensional momentum, $\QE$ is the energy density and $\QZ$ is the mass fraction of the chemical reactant.
In our case, the chemical reactant is unburnt gas.
The rows of \cref{eq:equation-set} are the conservation of mass, the conservation of momentum, the conservation of energy and the continuity equation of the reactant.
We split the flux into a hyperbolic $\hyperFlux(\Q)$ and a viscous part $\viscFlux(\Q, \gradQ)$
\begin{equation}
  \label{eq:flux}
  \flux(\Q, \gradQ) = \hyperFlux(\Q) + \viscFlux(\Q, \gradQ).
\end{equation}

The hyperbolic flux is given by
\begin{equation}
  \label{eq:hyper-flux}
  \hyperFlux(\Q) = \hyperFluxDef,
\end{equation}
which are the Euler equations coupled with an advection equation.
In this equation, $\bm{a} \otimes \bm{b} = \bm{a} \bm{b}^\intercal$ is the Kronecker or outer product of two vectors $\bm{a}$ and $\bm{b}$.
The pressure $\pressure$ is given by the equation of state of an ideal reacting gas
\begin{equation}
  \label{eq:eos}
  \pressure = (\gamma - 1) \left(\QE - \frac{1}{2} \left(\Qv \cdot \Qj \right)  - q_0 \QZ - gz \right).
\end{equation}
The term $q_0 \QZ$ is the chemical energy with a heat release $q_0$~\cite{helzel2000modified}, the term $gz$ is the geopotential height~\cite{giraldo2008study}.
We set $g$ to 9.81.
The pressure is related to the temperature $T$ by the thermal equation of state
\begin{equation}
  \label{eq:temperature}
  \pressure = \Qrho R T,
\end{equation}
where $R$ is the specific gas constant of a fluid.

The viscous flux is
\begin{equation}
  \label{eq:visc-flux}
  \viscFlux(\Q, \gradQ) = \viscFluxDef,
\end{equation}
where $\stressT$ and $(\kappa \nabla T)$ denote the stress tensor and heat flux respectively.
The viscous effects of the fluid are modeled by the stress tensor
\begin{equation}
  \label{eq:stress-tensor}
  \stressT(Q, \nabla Q) =
  \mu
  \left(
  \left(\nicefrac{2}{3} \divergence{\Qv} \right) -
  \left( \gradient{\Qv} + \gradient{\Qv}^\intercal \right)
  \right).
\end{equation}

We further introduce the ratio of specific heats $\gamma$ and the heat fraction for constant volume $c_v$ and constant volume $c_p$.
These constants are all fluid dependent and relate to each other~\cite{dumbser2010arbitrary} and the gas constant by
\begin{align}
  \label{eq:fluid-constants}
  \begin{split}
  c_v &= \frac{1}{\gamma - 1} R \\
  c_p &= \frac{\gamma}{\gamma - 1} R\\
  R &= c_p - c_v\\
  \gamma &= \frac{c_p}{c_v}.
  \end{split}
\end{align}
We can compute the heat conduction coefficient
\begin{equation}
  \label{eq:heat-conduction-coeff}
  \kappa = \frac{\mu \gamma}{\Pr} \frac{1}{\gamma - 1} R = \frac{\mu c_p}{\Pr},
\end{equation}
where the Prandtl number $\Pr$ depends on the fluid.
The temperature gradient $\gradient{T}$ in terms of the conservative variables can be found in \cref{chap:appendix-pde}.

The maximal absolute eigenvalues for the normal Jacobians of both convective and viscous part in direction of a normal vector $\normal$ are
\begin{align}
  \begin{split}
    \maxConvEigen &=  \vert \Qv_{\normal} \vert + c,\\
    \maxViscEigen &= \max \left( \frac{4}{3} \frac{\mu}{\Qrho},
                        \frac{\gamma \mu}{\Pr \Qrho}\right)
  \end{split}
\end{align}
with velocity in direction of the normal $\Qv_{\normal}$ and speed of sound $c = \sqrt{\gamma R T }$.
The Jacobians of both fluxes can be found in \cref{chap:appendix-pde}.

We also support two different kind of optional source terms.
The first source term is gravity, given by
\begin{equation}\label{eq:source-gravity}
  \source[\Qj] = - g \bm{e_z} \cdot \Qj,
\end{equation}
where $\bm{e_z}$ is the unit vector pointing in $z$-direction~\cite{giraldo2008study}.

\newcommand{\backgroundPressure}{\overline{\pressure}}
\newcommand{\backgroundRho}{\overline{\Qrho}}
Some of our scenarios can be described as a pertubation of a background state that is initially in hydrostatic balance
\begin{equation}
  \label{eq:hydrostatic-balance}
  \pdv{}{z} \backgroundPressure{\left (z \right )} = -g \backgroundRho(z),
\end{equation}
where $\backgroundPressure$ and $\backgroundRho$ is the background pressure and density.
We now focus on the following part of the momentum equation (neglecting viscous effects) of \cref{eq:equation-set} in combination with gravitational source term~\pcref{eq:source-gravity}
\begin{equation}
  \label{eq:momentum-equation}
  \pdv{\Qj}{t}+ \divergence{ \left(
    \Qv \otimes \Qj + \bm{I} \pressure
    \right)
  }
  =
  -g \bm{k} \cdot \Qrho.
\end{equation}
The background state \cref{eq:hydrostatic-balance} can be comparitively large and can thus diminish other parts of the flux due to numerical instabilities~\cite{muller2010adaptive}.
To avoid this, we split the pressure as $\pressure = \backgroundPressure(z) + \pressure' (\bm{x}, t)$ and the density as $\Qrho = \backgroundRho(z) + \Qrho'(\bm{x}, t)$.
Inserting this splitting and the definition of divergence into \cref{eq:momentum-equation} allows us to apply \cref{eq:hydrostatic-balance}.
The derivatives of the background states are non-zero only for the $z$-direction.
We arrive at 
\begin{equation}
  \label{eq:momentum-equation-split}
  \pdv{\Qj}{t}+ \divergence{ \left(
    \Qv \otimes \Qj + \bm{I} \pressure'
    \right)
  }
  =
  -g \bm{k} \Qrho'.
\end{equation}
We thus cancel parts of the flux with parts of the source term.
This form of the equation is inspired by equation set 3 of~\cite{giraldo2008study}.
They additionally use a splitting of the energy and use the pertubations directly as conservative variables.
%We do not use this to simplify the implementation of gradients.

The chemical reactive source term is
\begin{equation}\label{eq:source-reaction}
  \source[\QZ] = - K(T) \QZ.
\end{equation}
We use the simple reaction model
\begin{equation}\label{eq:reaction-model}
\begin{cases}
  - \frac{1}{\reactionTimescale} & T > \reactionTemperature\\
  0 & \text{otherwise}
\end{cases}  
\end{equation}
where $\reactionTimescale$ is the timescale of the reaction and $\reactionTemperature$ is the activation temperature~\cite{hidalgo2011ader,helzel2000modified}.

\sidetitle{Boundary conditions}
To close the system, we need to impose boundary conditions.
Many scenarios are described with periodic boundary conditions.
This is not possible with \exahype{}.
Instead, we use the analytical solution of our problems at the boundary, imposing both the conservative variables $\Q$ and its gradient $\gradQ$.
This type of boundary condition is called Cauchy condition.
Note that this leads to an error when our problem does not possess an exact analytical solution.
This is the case for test cases that are exact solutions for the incompressible Navier Stokes equations but do not satisfy the compressible equation set.

As a physical boundary condition, we limit ourselves to wall boundary condition.
A standard wall boundary condition for viscous fluid is the no-slip condition, where we assume that when the fluid is close to a wall, it has a velocity of zero relative to the wall.
We enfore this by setting
\begin{align}
  \label{eq:no-slip}
  \begin{split}
  \Qrho^o &= \Qrho^i, \\
  \Qj^o &= 2 \Qj^w - \Qj^i, \\
  \QE^0 &= \QE^i,\\
  \QZ^0 &= \QZ^i,\\
  {(\nabla Q)}^o &= {(\nabla Q^o)},
  \end{split}
\end{align}
where a superscript of $o$ and $i$ denotes the values outside and inside of the domain respectively.
The term $\Qj^w$ denotes the velocity of the wall.

Similarly to the no-slip condition, we define the free-slip condition
\begin{equation}
  \label{eq:free-slip}
  \Qj^o_d = \begin{cases}
    -\Qj^i_d & d = \text{normal} \\
    \Qj^i_d & \text{otherwise}
    \end{cases}
\end{equation}
where only the velocity in normal direction is zero next to the wall.
The other variables are extrapolated in the same manner as in \cref{eq:no-slip}.

We manually set the energy component of the numerical flux to zero for both no- and free-slip conditions.
\section{The ADER-DG Method}\label{sec:ader-dg}
We describe the arbitrary derivative discontinous Galerkin (\aderdg) method in this chapter.
Our description of the main method follows~\cite{dumbser2008unified,dumbser2010arbitrary,dumbser2018efficient}, our notation follows primarily~\cite{dumbser2018efficient}.
Let $\NVar$ be the number of variables, $\dimensions$ the number of spatial dimensions and $N > 0$ the degree of the basis functions.
We also call $N$ the order of the method.
We discuss the approximation of systems of partial differential equations (\pde) of the form
\begin{equation}
  \label{eq:conservation-law-gradient}
 \frac{\partial}{\partial_t}  \Q + \div \flux(\Q, \gradQ) = \source(\bm{x}, t, \Q),
\end{equation}
which in contrast to hyperbolic conservation laws of the form
\begin{equation}
  \label{eq:conservation-law}
 \frac{\partial}{\partial_t}  \Q + \div \flux(\Q) = \source(\bm{x}, t, \Q)
\end{equation}
contain the gradient $\gradQ \in \mathbb{R}^{\NVar \times \dimensions}$ of the so called vector of conservative variables $\Q \in \mathbb{R}^{\NVar}$~\cite{dumbser2010arbitrary}.
They are therefore not hyperbolic but rather parabolic or elliptic.

We discuss the solution of a conservation law \cref{eq:conservation-law-gradient} with two or three dimensional domain $\domain$ and its boundary $\boundary$.
In our implementation of the discontinous Galerkin (\dg) framework, we approximate this solution in the space
\begin{equation}
  \label{eq:dg-space}
  \broken = \bigcup_k \cell[k]
\end{equation}
of disjoint quadrilateral cells $\cell$.
Note that we do not distinguish between the approximation space and the domain; the use should be clear given its surrounding context.
For each cell we denote its center by $\cellCenter (\cell)$ and its size by $\Delta x, \Delta y$ and $\Delta z$.

The \aderdg{}-scheme is a predictor-corrector method.
We first compute a local time evolution of each cell in a predictor step and then add the influence of the neighbors in the corrector step.
\sidetitle{Reference Cell}
For reasons of computational efficiency, we describe the scheme in terms of a reference cell.
We use the quad cell $\hat{\cell} = (0, 1)^\dimensions$ as space reference cell.
The mapping, which takes a point on the reference cell with coordinates $\hat{\bm{x}}$ and and returns its coordinates $\bm{x}$ in a grid cell $\cell$ is
\begin{equation}\label{eq:space-mapping}
  \bm{x} = \mapping (\hat{\bm{x}}) =
  \cellCenter(\cell) +
\begin{pmatrix}
\Delta x && \\
&\Delta y & \\
&&\Delta z&
\end{pmatrix}
  \begin{pmatrix}
    \hat{x} - 0.5\\
    \hat{y} - 0.5\\
    \hat{z} - 0.5
  \end{pmatrix}.
\end{equation}
The symbol $\mapping^{-1}$ denotes the inverse mapping.
It can be used to define a function $f(\bm{x})$ acting on a point $\hat{\bm{x}}$ of a regular cell in terms of a function $\hat{f}(\bm{x})$ that acts on the corresponding point of the reference cell by the relation
\begin{equation}
  \label{eq:function-ref-cell}
  f(\bm{x}) = f\left( \mapping^{-1}(\bm{x}) \right) = \hat{f}(\hat{\bm{x}}).
\end{equation}

Similar to this, we define a mapping that maps a reference time point $\hat t$ to simulation time $t$
\newcommand{\timeMapping}{\mathcal{T}}
\begin{equation}
  \label{eq:time-mapping}
  t = \timeMapping(\hat{t}) = \curTime + (\Delta t) \hat{t},
\end{equation}
where the reference time interval is $[0,1]$ and the time interval of a cell is $[\curTime, \nextTime]$.
The time at the beginning of a timestep with index $k$ and timestep size $\Delta t$ is denoted by $\curTime$, the time at the end of a step is $\nextTime = \curTime + \Delta t$.

The Jacobian determinant of $\mapping$ is simply the volume $V$ of the cell
\begin{equation}
  \label{eq:determinant-mapping}
  \volume = \operatorname{det}(\operatorname{Jacobian}\mapping) = \Delta x \, \Delta y \,\Delta z,
\end{equation}
the Jacobian determinant of $\timeMapping$ is $\Delta t$.
\todo{Similarly -> as well as?}
This is useful for evaluating derivatives by the chain rule and, similarly, integrals by substitution.
For example, we can evaluate a volume integral over a cell by
\begin{equation}
  \label{eq:integration-by-substitution}
  \intdcell{
f(\bm{x})
  }
  =
\volume \intdrefcell{
    \hat{f}(\hat{\bm{x}})
  }.
\end{equation}
\sidetitle{Basis Functions}
The next step is the definition of basis functions for our cells.
We use Langrange polynomials that are defined in the one-dimensional case for the reference interval $(0,1)$ by
\begin{align}
  \label{eq:lagrange-basis}
    \lagrangeRef(x) = \prod_{\substack{0 \leq j \leq N \\ i \neq j}} \frac{x - {\quadNode[j]}}{x_i - \quadNode[j]}.
\end{align}
We can then approximate functions as a sum over the support points $\quadNode[j]$
\begin{equation}
  \label{eq:langrange-expansion-1d}
  f(x) = \sum_{i = 0}^N f(x_i) \lagrangeRef[i](x) = f(x_i) \lagrangeRef[i](x),
\end{equation}
where we used the Einstein summation convention which implies summation over repeated free indices.
This convention is used from here on.
The support points $\quadNode[i]$ are chosen such that they coincide with the nodes of the Gauss-Legendre quadrature of order $N+1$ which we can use to evaluate integrals
\begin{equation}
  \label{eq:quadrature-1d}
  \int_0^1 f(\hat{x}) \dd{\hat{x}} = \sum_{i=0}^{N} \quadWeight[i] f(\quadNode[i]),
\end{equation}
where $\quadWeight[i]$ is the quadrature weight corresponding to the node $\quadNode[i]$.

The basis functions are similarly defined in $\dimensions$-dimensions by considering nodes that are the tensor-products of $d$ one-dimensional Gauss-Legendre quadrature nodes.
They are indexed by a multi-index $n = (n_1, \ldots, n_\dimensions)$
\begin{equation}
  \label{eq:basis-space}
  \sbasisRef[][n] (\bm{x}) = \prod_{i=0}^{\dimensions} \lagrangeRef[n_i] (x_i)
\end{equation}
that is linearised such that $n = (0, \ldots, \sbasisSize)$.
A $d$-dimensional function is then approximated by
\begin{equation}
  \label{eq:lagrange-expansion}
  f(\bm{x}) = \sum_{i = 0}^{\sbasisSize - 1} f(\bm{x}_i) \lagrangeRef[i](\bm{x}) = f(\bm{x}_i) \lagrangeRef[i](\bm{x}).
\end{equation}
Similarly to the one-dimensional case, we can evaluate integrals by
\begin{align}
  \label{eq:quadrature}
  \underbrace{\int_0^1 \cdots \int_0^1}_{\dimensions \text{ times}} f(\hat{\bm{x}}) \dd{\hat{\bm{x}}} &=
 \sum_{n=0}^{\sbasisSize-1} \quadWeight[n] f(\quadNodeNd[n]) \\
  \intertext{with}
  \quadWeight[n] &= \prod_{i}^{d} \quadWeight[n_i],
\end{align}
where the quadrature weights $\quadWeight[n]$ are the product of all relevant one-dimensional quadrature weights, using the aforementioned (linearized) multi-index $n$.

Two important properties of this family of polynomials are
  \begin{alignat}{10}
    \label{eq:basis-interpolating}
&& \text{Interpolating:} \qquad && \lagrangeRef[i] (\quadNode[j]) &= \delta_{ij},\\
&& \qquad && \sbasisRef[][n] (\quadNodeNd[m]) &= \delta_{nm}, \notag\\
\label{eq:basis-orthogonality}
&& \text{Orthogonality:} \qquad && \int_0^1 \lagrangeRef[i](\quadNode[i]) \lagrangeRef[j](\quadNode[j]) \dd{\quadNode[j]} &= \quadWeight[i] \delta_{ij},\\
&& \qquad &&
\underbrace{\int_0^1 \cdots \int_0^1}_{d \,\text{times}}
\sbasisRef[][n] \left( \hat{\bm{x}} \right)
\sbasisRef[][m] \left( \hat{\bm{x}} \right) \dd{\hat{\bm{x}}}
&= \quadWeight[n] \delta_{nm}, \notag
    \end{alignat}
    where we made use of the Kronecker delta
\begin{equation}
      \label{eq:kronecker-delta}
      \delta_{ij} =
      \begin{cases}
        1 & \text{if } i = j\\
        0 & \text{otherwise}
      \end{cases}.
\end{equation}

We define the basis functions over a space-cell with an index $n = 0, \ldots, \sbasisSize - 1$
\begin{align}\label{eq:space-basis}
  \begin{split}
  \sbasis[][n] (\bm{x}) &= \sbasisRef[][n] \left( \mapping^{-1}(\bm{x}) \right).
  \end{split}
\end{align}
In addition to the space cells, we also introduce space-time cells $\cell \times [\curTime, \nextTime]$ with corresponding reference cell $(0,1)^\dimensions \times  [0,1]$.
A basis for these cells is given by
\begin{align}\label{eq:space-time-basis}
  \begin{split}
  \stbasisRef[][l,n](\hat{\bm{x}}, \hat{t}) &= \lagrange[l](t) \sbasis[][n] (x)\\
  \stbasis[][l,n] (\bm{x}, t) &= \stbasisRef \left( \mapping^{-1}(\bm{x}), \timeMapping^{-1}(t) \right)
  \end{split}
\end{align}
where the first index $l = 0, \ldots, N$ runs over the time.
Outside of elements, we define the basis functions to be equal to zero.

We introduce an additional index $v = (0, \ldots, \NVar - 1)$ that corresponds to a variable.
The complete basis functions are then defined by
\begin{align}
  \begin{split}
    \stbasis[][v,n,l] (\bm{x},t) &= \bm{e_v} \otimes \stbasis[][n,l] (\bm{x}, t) \\
    \sbasis[][v,n] (\bm{x}) &= \bm{e_v} \otimes \sbasis[][n] (\bm{x})
  \end{split}
\end{align}
where $\bm{e_v} \in \mathbb{R}^{\NVar}$ is vector with components
\begin{equation}
  \left( \bm{e_v} \right)_{v'} = \delta_{v v'},
\end{equation}
and $\bm{A} \otimes \bm{B}$ denotes the tensor product between two tensors $\bm{A}$ and $\bm{B}$.

\sidetitle{Expansion of quantities}\todo{Maybe explain this later, right now $\stpredictor$ is not defined!}
All quantities can be expanded in the space-time basis.
For example, the expansion of the space-time predictor $\stpredictor[\cell]$ can be written as
\begin{equation}
  \label{eq:cell-approx-space-time}
  \stpredictor[\cell] (\bm{x}, t) = \stpredictorCoeff[\cell][v',l',n'] \stbasis[\cell][v',l',n'](\bm{x}, t).
\end{equation}
We denote the space-time flux as $\stflux$ and the space-time source as $\stsource$ with coefficient vectors $\stfluxCoeff$ and $\stsourceCoeff$.
Similarly, we define the space degrees of freedoms by an expansion in the space basis, for example for the space predictor
\begin{equation}
  \label{eq:cell-approx-space}
  \spredictor[\cell] (\bm{x}) = \spredictorCoeff[\cell][v',n'] \sbasis[\cell][v',n'] (\bm{x}),
\end{equation}
where the basis does not depend on the time, unlike in \cref{eq:cell-approx-space-time}.
We need the space-expansion for the predictor $\spredictor$, the discrete solution $\ssol$, the source $\ssource$ and the flux $\sflux$,
with coefficients $\spredictorCoeff, \ssolCoeff, \ssourceCoeff$ and $\sfluxCoeff$ respectively.
Each coefficient index generally has the same range as the corresponding basis index.
For example, the indices $n$ and $n'$ both run over the entire space-basis.

Here, the computational advantage of the nodal approach is evident:
The coefficients correspond to the quantity evaluated at the basis node, due to the interpolation property~\pcref{eq:basis-interpolating}.

\sidetitle{Predictor}
We are now ready to derive the predictor.
We first multiply the conservation law by a test function of the same function space as the basis, integrate over a space-time-element $\cell$ and replace all terms with their discrete approximations.
In particular, we replace the vector of conservative variables $\Q$ with the space-time predictor $\stpredictor[\cell]$.
\begin{align}\label{eq:weak-pde-space-time}
\begin{split}
\intdt{\intdcell{
    \sttestfunction[\cell][v,l,n](\bm{x}, t)
    \pdv{\stpredictor}{t}
}}
&+
\intdt{\intdcell{
    \sttestfunction[\cell][v,l,n](\bm{x}, t)
    \left(
      \divergence{\stflux(\stpredictor, \gradient{\stpredictor})}
    \right)
}}
\\&=
\intdt{\intdcell{
  \sttestfunction[\cell][v,l,n] \stsource(\bm{x}, t, \stpredictor)
}}.
\end{split}
\end{align}
%where the indices $v = (0, \ldots, \NVar-1)$, $n = (0, \ldots, \sbasisSize - 1)$ and $l = (0, \ldots, N)$ run over the variables, space and time.

Henceforth, we drop the cell index for all quantities and the spatial argument for the basis and test functions.
We integrate the first term of \cref{eq:weak-pde-space-time} by parts in time and the flux divergence in space.
Here we do not use the Riemann solver for the flux boundary term but rather use the known discrete solution at time $t$.
This corresponds to upwinding in time~\cite{dumbser2008unified}.
Note that this neglects the interaction with neighbouring cells.
The cell-local scheme is then
\begin{align}\label{eq:space-time-predictor}
\begin{split}
\intdcell{
  \sttestfunction[][v,l,n] (\nextTime) \stpredictor[i+1](\nextTime)
}
&-
\intdt{\intdcell{
    \pdv{}{t} \sttestfunction[][v,l,n] (t) \stpredictor[i+1](t)
}}
= 
\intdcell{
  \sttestfunction[][v,l,n] (\curTime) \ssol (\curTime)
}
\\&+
\intdt{\intdcell{
    \sttestfunction[][v,l,n] (t) \divergence{\stflux(\stpredictor[i], \gradient{\stpredictor[i]})}
}}
\\&+
\intdt{\intdcell{
    \sttestfunction[][v,l,n] (t) \stsource(\stpredictor[i])
}}.
\end{split}
\end{align}
Here, we introduced the iteration counter $i$ as a subscript for the space-time predictor.
Using the expansions in space and time~\pcref{eq:cell-approx-space,eq:cell-approx-space-time} results in a local systems of equations that can be solved by a fixed-point iteration scheme.
We set the initial value for the space-time predictor as
\begin{equation}
  \label{eq:initial-guess}
\quad \stpredictorCoeff[0][v,l,n] = \ssolCoeff[][v,n]
\end{equation}
which corresponds to a stationary initial guess~\cite{dumbser2008unified}.
For an overview of possible initial guesses, see~\cite{dumbser2018efficient}.

% To show the computational efficiency of our scheme, we follow the approach of~\cite{dumbser2008unified} and collect our integrals into matrices\todo{Maybe mention that they are actually unrolled tensors?\\Source here is Dominics \aderdg{}-document.}.
% We arrive at the cell-local scheme
% \begin{align}
%    \label{eq:predictor-iterative}
%    \pleft \stpredictorCoeff[i+1] &= \prightsol (\ssol) - \prightpred(\stpredictorCoeff[i]) \\
%   \stpredictorCoeff[i+1] &= \pleft^{-1} \left( \prightsol (\ssol) - \prightpred(\stpredictorCoeff[i]) \right).
%  \end{align}
% where we introduced the matrices
% \begin{equation}
%   \label{eq:matrices}
%   \pleft \in \mathbb{R}^{\NVar \stbasisSize \times \NVar \stbasisSize}, \quad
%   \prightsol \in \mathbb{R}^{\NVar \stbasisSize}, \quad
%   \prightpred \in \mathbb{R}^{\NVar \stbasisSize}
% \end{equation}
% with entries
% \begin{align}
%   \label{eq:predictor-matrices}
% \begin{split}
%   \pleft_{(v l n), (v l' n')} &=
% \intdcell{
%   \sttestfunction[][v,l,n](\nextTime) \  \stbasis[][v,l',n'](\nextTime)
% }
% \\&-
% \intdt{\intdcell{
%   \left( \pdv{}{t} \sttestfunction[][v,l,n] (t) \right) \stbasis[][v,l',n'] (t)
% }}
% \end{split}
% \\
% \begin{split}
% \prightsol_{v,l,n} (\ssol) &=
% \intdcell{
%   \sttestfunction[][v,l,n] (t) \sbasis[][v,n'] \ssolCoeff[][v,n']
% }
% \\&+
% \intdt{\intdcell{
%   \sttestfunction[][v,l,n] (t) \stbasis[][v,l',n'](t) \stsourceCoeff[][v,l',n'](\stpredictor[i])
% }}                             
% \end{split}
%   \\
% \prightpred_{v,l,n} (\stpredictor) &=
% \stfluxCoeff[][v,l',n'] (\stpredictor)
% \intdt{\intdcell{
%   \left( \gradient{\sttestfunction[][v,l,n]} \right) \stbasis[][v,l',n'](t)
% }}
% \end{align}
% Note here that $\prightpred$ is constant during a interation step.
In practice, we precompute all integrals that contain only basis functions and their derivatives for the reference cell.
The computation of the integrals for general space-time cells can then be performed by applying \cref{eq:integration-by-substitution}.
For details and proof of convergence for the case of linear \pde{}s, see~\cite{dumbser2008unified}.

\sidetitle{Preparing for corrector}
It is useful to compute time-averaged versions of our quantities.
For example, this can be done for the space-time predictor by
\begin{align}\label{eq:time-average}
  \begin{split}
  \spredictor (\bm{x}) &= \frac{1}{\Delta t} \intdt{
\stpredictor(\bm{x}, t),
  } \\
\spredictor[][v,n] (\bm{x}) &= \stpredictorCoeff[][v,l,n] \quadWeight[l].
  \end{split}
\end{align}
We then extrapolate the space predictor and its gradient, and the fluxes to the boundary.
In our case we have a $\dimensions - 1$ dimensional basis for all $2 \dimensions$ faces.
We accomplish this by evaluating the function at the degrees of freedom at the boundary.
Due to the interpolating property of the nodal basis~\pcref{eq:basis-interpolating}, this reduces to a change of basis which can be precomputed on the reference element.
We use the extrapolated values later for the Riemann solver and for the reconstruction of boundary values.

\sidetitle{Corrector}
The final step of the \aderdg{}-scheme is the corrector.
We multiply the system \cref{eq:conservation-law-gradient} by a space test function and integrate over the space-time control volume.
This results, yet again, in the weak formulation of the \pde{}
\begin{equation}
  \label{eq:weak-pde}
\intdt{\intdcell{
\stestfunction[][v,n] \pdv{\Q}{t}
}}
+
\intdt{\intdcell{
    \stestfunction[][v,n] \left( \div{F(\Q, \gradQ } \right)
}}
=
\intdt{\intdcell{
    \stestfunction[][v,n] S(\Q, \bm{x}, t)
}}.
\end{equation}
We now insert our discrete approximations and apply the space-basis expansion~\pcref{eq:cell-approx-space} to the discrete solution.
The first term can be integrated by parts in time.
This has the convenient result that only the boundary terms remain because the basis is constant in time.
We then integrate the flux divergence by parts in space and insert the Riemann solver to connect with the neighbouring elements.
Finally, we need to solve
\newcommand{\massMatrixDef}{\intdcell{
  \stestfunction[][v,n] \sbasis[][v,n']
}}%
\begin{align}
\begin{split}
\label{eq:corrector}
\left(
\massMatrixDef
\right)
(\ssolCoeff[\nextTime][v,n'] - \ssolCoeff[\curTime][v,n'])
&+
\left(\intdt{\intdcellb{
      \stestfunction[][v,n] \Riemann(\stpredictor[-], \gradient{\stpredictor[-]}; \stpredictor[+], \gradient{\stpredictor[+]}) \cdot \normal
}}\right)
\\&-
\left(\intdt{\intdcell{
    \gradient{\stestfunction[][v,n]} \cdot  \stflux(\spredictor, \gradient{\spredictor})
}}\right)
\\&=
\left(\intdt{\intdcell{
      \stestfunction[][v,n] \stsource(\spredictor)
}}\right)
\end{split}
\end{align}
for the coefficients $\ssolCoeff[\nextTime][v,n]$ of the new solution.
Here the superscripts $\pm$ denote values that are extrapolated to the boundaries.
Again, the computational effort can be vastly reduced by collecting the integrals into matrices and precomputing as much as possible.
Using the orthogonality of the basis functions~\pcref{eq:basis-orthogonality} on the first term of \cref{eq:corrector}, it becomes clear that the correction is a simple, explicit computation.
% The first term is the mass tensor with entries
% \newcommand{\massMatrix}[1][]{\bm{M}_{#1}}
% \begin{equation}
%   \label{eq:mass-matrix}
%   \massMatrix[v n n'] = \massMatrixDef = \volume \underbrace{\quadWeight[n] \delta_{n n'}}_{\text{diagonal}}
% \end{equation}
% which is, due to the orthogonal basis \pcref{eq:basis-orthogonality}, diagonal for each variable $v$ and thus trivial to invert.
% We can then insert the definition of our basis function into our equation and arrive at
% \begin{align}
%   \massMatrix \left( \ssolCoeff[t+1] - \ssolCoeff[t] \right) &=
%   \Delta t \left( \bm{a} - \bm{b} + \bm{s} \right) 
%   \intertext{with}
% a_{v,n} &= \sfluxCoeff[][n'] \intdcell{
%   \sbasis[][v,n'] \gradient{\sbasis[][v,n]}(t)
% }
% \\
% \label{eq:corrector-surface}
% b_{v,n} &= \intdcellb{
%     \stestfunction[][v,n] \Riemann \left( \spredictor[-], \gradient{\spredictor[-]}; \spredictor[+], \gradient{\spredictor[+]} \right) \cdot \normal
% } \\
% s_{v,n} &=
% \ssourceCoeff[][v,n'] \intdcell{
%   \sbasis[][v,n] \sbasis[][v,n']
% }
% ,
% \end{align}
% where we evaluated the time-integrals by \cref{eq:integration-by-substitution}.
% All quantities are now time-averaged~\pcref{eq:time-average} and are represented in the space basis~\pcref{eq:cell-approx-space}.
Note that we evaluate the Riemann problem for the time-averaged space-predictor in our implementation.
This is justified by the linearity of our numerical flux~\cite{dumbser2008unified,dumbser2010arbitrary}.
% The surface integral \pcref{eq:corrector-surface} can be evaluated by summing over the contributions of the faces of each cell, which are computed by integrating the extrapolated space predictor with Gaussian quadrature.
% Finally, we can solve \cref{eq:corrector} for the new unknows $\ssolCoeff[k+1]$
% \begin{align}\label{eq:update-predictor}
% \begin{split}
%   %\bm{u}^{n+1}
%   \ssolCoeff[k+1][v]
%   &= \ssolCoeff[k][v] + \Delta t (\operatorname{update}) \\
%   \operatorname{update} &= \massMatrix[v]^{-1} \left( \bm{a}_v - \bm{b}_v + \bm{s}_v \right)
% \end{split}
% \end{align}
% which is clearly of the form of a one-step scheme.

\sidetitle{Riemann solver \textit{\&} timestep}
As a Riemann solver we use a simple Rusanov-flux that is adapted for diffusive problems
\begin{equation}
  \label{eq:rusanov-flux}
  \begin{split}
  \Riemann(\Q^-, \gradQ^-; \Q^+, \gradQ^+) \cdot \normal &=
  \frac{1}{2} \left(
    \flux(\Q^+, \gradQ^+) +
    \flux(\Q^-, \gradQ^-)
  \right)
  \\&-
  \frac{1}{2} s_\text{max} (\Q^+ - \Q^-),
  \end{split}
\end{equation}
with a penalty term
\begin{equation}
  \label{eq:parabolic-penalty}
  s_\text{max}  = \max \left(
\maxConvEigen[\Q^-], \, \maxConvEigen[\Q^+]
\right) +
2 \eta \max \left(
\maxViscEigen[\Q^-], \, \maxViscEigen[\Q^+]
\right)
\end{equation}
and
\begin{equation}
  \eta = \frac{2N+1}{h \sqrt{\frac{1}{2} \pi}}.
\end{equation}
In this equation $h$ is the side-length of an element and a superscript of $\pm$ denotes the state of the right or left side.
The penalty term depends on the maximal absolute eigenvalues of the Jacobian matrices in normal direction of both convective and viscous fluxes
\begin{align}
  \begin{split}
    \maxConvEigen &= \max \quad \operatorname{eigenvalues} \left( \pdv{\flux}{\Q} \cdot \normal  \right),\\
    \maxViscEigen &= \max \quad \operatorname{eigenvalues} \left( \pdv{\flux}{\left( \gradQ \cdot \normal \right)} \cdot \normal \right),
  \end{split}
\end{align}
in direction of the normal vector $\normal$. 
This Riemann solver was first published in~\cite{gassner2008discontinuous} and used for an \textsc{ader-dg} scheme in~\cite{dumbser2010arbitrary}.

The timestep is restriced to a so-called \textsc{cfl}-type penalty
\begin{equation}\label{eq:cfl-aderdg}
 %\Delta t \leq  \text{CFL} \, \frac{\alpha(N) \, h}{\maxConvEigen + 2 \maxViscEigen \frac{1}{\alpha(N) h}}
 \Delta t \leq  \text{CFL} \, \alpha(N) \, h \sum_{i=1}^{\dimensions} \frac{1}{\maxConvEigen_i + 2 \maxViscEigen_i \frac{1}{\alpha(N) h}},
\end{equation}
where the eigenvalues are evaluated for the flux Jacobian in direction $i$~\cite{dumbser2010arbitrary,gassner2008discontinuous}.
The constant $\alpha(N) \leq {\left( 2N+1  \right)}^{-1}$ can be obtained from von Neumann analysis on a simple model problem and depends on the approximation order~\cite{dumbser2008unified}.
We use this constant for both hyperbolic and diffusive penalty terms to ensure stability.
For a more detailed and rigorous stability analysis of the Riemann solver, see~\cite{gassner2008discontinuous}.
We use a value of $0.7$ for the constant $\text{CFL}$.

\section{MUSCL-Hancock Finite Volume Scheme}\label{sec:muscl}
We discussed a modern, high-order \dg{} method in the previous chapter.
While this scheme is efficient, it can become unstable, especially in case of discontinuities.
A simple alternative is a finite volume scheme.
We use the \muscl{} method\cite{vanLeer1979towards}.
Our discussion and notation follows the one in~\cite{toro2009riemann}.
Similar to~\cite{toro2009riemann} we only describe the two-dimensional case without source terms, an extension to three dimensions is straigthforward.

In contrast to the previous \dg{} scheme, we now only store averages instead of a full polynomial solution for each cell.
We achieve higher order by reconstructing a linear function from a cell and its neighbours.
Let $\cellAvg$ denote the averages per cell at spatial coordinates $i,j$.

\sidetitle{Boundary Extrapolated Values}
We first compute the slope of the linear function connecting the cells.
We use the minmod slope-limiter
\begin{equation}
  \label{eq:minmod}
  \minmod(a, b) =
  \begin{cases}
    0.0 & \sign(a) \neq \sign(b) \\
      a & \vert a \vert < \vert b \vert \\
      b & \text{otherwise}
  \end{cases}
\end{equation}
to avoid unphyiscally steep gradients and thus to stabilize the scheme~\cite{leVeque2002finite}.
This function can be used to compute the slopes
\begin{align}\label{eq:slopes}
  \begin{split}
   \slope{x} &=  \Delta x \minmod \left( \cellAvg[i+1,j] - \cellAvg[i,j], \, \cellAvg[i,j] - \cellAvg[i-1, j] \right),\\ 
   \slope{y} &=  \Delta y \minmod \left( \cellAvg[i,j+1] - \cellAvg[i,j], \, \cellAvg[i,j] - \cellAvg[i, j-1] \right),
   \end{split}
\end{align}
where $\Delta x$ and $\Delta y$ are the inverse cell sizes in $x$ and $y$ direction respectively.
Here, we apply the $\minmod$ function element-wise.

We use the slope to reconstruct the value at the cell boundaries.
In the following, $\cellAvg^{\pm x}$ is the average of the left ($+$) or right ($-$) cell boundary.
Similar, $\cellAvg^{\pm y}$ is the value at the top and bottom cell boundary.
These so called boundary extrapolated values are given by
\newcommand{\extrapolatedCellAvg}[3][i,j]{\cellAvg[#1]^{#3 #2} = \cellAvg #3 \frac{1}{2} \slope{#2}}%
\begin{equation}
\begin{alignedat}{2}
& \extrapolatedCellAvg{x}{-} , \qquad && \extrapolatedCellAvg{x}{+}, \\
& \extrapolatedCellAvg{y}{-} , \qquad && \extrapolatedCellAvg{y}{+}.
\end{alignedat}
\end{equation}
We also use the slopes defined in \cref{eq:slopes} to estimate the gradient of $\cellAvg$ in each cell by the block matrix
\begin{equation}
  \label{eq:muscl-gradient}
  \gradCellAvg = \left( \slope{x} \bigg\rvert \slope{y} \right),
\end{equation}
which is of course an abuse of notation insofar as it is not the gradient of the constant cell value but rather of the linear reconstruction \pcref{eq:slopes}.
\sidetitle{Time Evolution}
To achieve second order in time, we evolve the boundary extrapolated values
\newcommand{\evolvedCellAvg}[2][i,j]{\hat{U}_{#1}^{#2}}
\begin{equation}\label{eq:muscl-time-evolution}
  \begin{split}
    \forall (k \in \{-x, +x, -y, +y \} ): \quad  \hat{U}_{ij}^k &= \cellAvg^k
\\&+ \frac{\Delta t}{2 \Delta x} \left[ \fluxX(\cellAvg^{-x}, \gradCellAvg) - \fluxX(\cellAvg^{+x}, \gradCellAvg) \right]
\\&+ \frac{\Delta t}{2 \Delta y} \left[ \fluxY(\cellAvg^{-y}, \gradCellAvg) - \fluxY(\cellAvg^{+y}, \gradCellAvg) \right],
  \end{split}
\end{equation}
where $\fluxX$ and $\fluxY$ denote the $x$ and $y$ part of the flux respectively.
Note that we do not need to consider neighboring cells for this step.

Finally, the update can be described by
\begin{align}
  \begin{split}
    \cellAvg (t^{n+1}) &= \cellAvg (t^n)
    \\ &+
  \frac{\Delta t}{\Delta x}
    \Riemann \left(
      \evolvedCellAvg[i-1,j]{+x}, \gradCellAvg[i-1,j]; \ %
      \evolvedCellAvg[i,j\phantom{-1}]{-x}, \gradCellAvg[i,j\phantom{-1}]
    \right)
    \\ &-
  \frac{\Delta t}{\Delta x}
    \Riemann \left(
      \evolvedCellAvg[i,j\phantom{+1}]{+x}, \gradCellAvg[i,j\phantom{+1}]; \ %
      \evolvedCellAvg[i+1,j]{-x}, \gradCellAvg[i+1,j]
    \right)
    \\ &+
  \frac{\Delta t}{\Delta y}
    \Riemann \left(
      \evolvedCellAvg[i,j-1]{+y}, \gradCellAvg[i,j-1]; \ %
      \evolvedCellAvg[i,j\phantom{-1}]{-y}, \gradCellAvg[i,j\phantom{-1}]
    \right)
    \\ &-
  \frac{\Delta t}{\Delta y}
    \Riemann \left(
      \evolvedCellAvg[i,j\phantom{+1}]{+y}, \gradCellAvg[i,j\phantom{+1}]; \ %
      \evolvedCellAvg[i,j+1]{-y}, \gradCellAvg[i,j+1]
    \right).
  \end{split}
\end{align}
This results in a scheme that has an order of convergence of two for both space and time.
\todo{Is this enough detail?}
We treat the source term by a simple splitting scheme.
The interested reader is referred to the discussions in~\cite{leVeque2002finite,toro2009riemann}.

We again use the Rusanov-type Riemann solver~\pcref{eq:rusanov-flux}.
The timestep is subject to a penalty of the form
\begin{equation}\label{eq:cfl-muscl}
 \Delta t \leq  \text{CFL} \, \frac{h}{\maxConvEigen + 2 \maxViscEigen \frac{1}{h}},
\end{equation}
with a constant value of $\text{CFL} = 0.9$.

For reasons of simplicity, we only prescribe the solution at the boundary and then estimate the gradient with \cref{eq:muscl-gradient}.
This has the effect that the scheme becomes unstable in the case of strong viscous effect next to the boundaries.

\section{Adaptive Mesh Refinement \textit{\&} Finite Volume Limiting}\label{sec:grid}
This section is concerned with two different but related concepts:
finite volume limiting and adaptive mesh refinement.
Both have in common that they change the structure of the grid.
Instead of a regular grid where each cell has the same size, we now have different cell sizes and even different numerical schemes.

\subsection{Limiting}\label{sec:limiting}
Higher order \textsc{dg} methods cannot cope with discontinuous solutions.
Even worse, in the case of non-linear fluxes, discontinuities can appear even from smooth initial data.
In addition, oscillations stemming from the high-order polynomical approximation can lead to wrong or unphysical data.
This is called Gibbs phenomenon.
A classical way of dealing with this problem is the usage of so called limiters.
The idea is to smooth out steep gradients and thus lessen the effect of discontinuities~\cite{hesthaven2008nodal}.

A relatively recent way of dealing with this problem is the finite volume subcell limiter.
Our discussion follows the one in~\cite{dumbser2016simple}.
This limiting is an a posteriori limiting, which means that we first evaluate a timestep with an unlimited \aderdg{}-method and then check whether our solution is \enquote{correct}.
In our case, we check whether the solution is a finite floating point number.
We also check whether it is physically admissible with the criterion
\begin{equation}
  \label{eq:limiting-physical}
  \operatorname{is-admissible}(\Q) =
  \begin{cases}
    \text{true} & \Qrho > 0 \land \pressure > 0 \land \QZZ \geq 0 \\
    \text{false} & \text{otherwise}.
  \end{cases}
\end{equation}
The positivity of the pressure implies positivity of energy.

To combat the Gibbs phenomenon, we can make use of the discrete maximum principle (\textsc{dmp})~\cite{dumbser2016simple}.
This criterion marks cells as troubled that have a numerical solution that is significantly higher or lower than the solutions of its neighbors, which are denoted by the set $\mathcal{N}$.
Here, a cell is defined to be its own neighbor.
We check for each cell $\cell[i]$ whether
\begin{align}\label{eq:limiting-dmp}
\forall \bm{x} \in \cell[i]:  &\min_{\bm{y} \in \mathcal{N}_i} \left( \ssol[\curTime](\bm{y}, \curTime) \right) - \delta \leq \ssol[\nextTime](\bm{x}, \nextTime)
  \leq \max_{\bm{y} \in \mathcal{N}_i} \left( \ssol(\bm{y}, \curTime) \right) + \delta
\intertext{with tolerance}
\delta &= \max \left(
  \varepsilon_0, \varepsilon \left(
         \max_{\bm{y} \in \mathcal{N}} \left( \ssol[\curTime](\bm{x},\curTime) \right) -
         \min_{\bm{y} \in \mathcal{N}}\left( \ssol[\curTime](\bm{x},\curTime) \right)
  \right)
  \right)
\end{align}
holds.
The parameters $\varepsilon$ and $\varepsilon_0$ can be set freely.
The former controls by which factor the local jump can differ from its neighbourhood, the latter is needed for zero jumps~\cite{dumbser2016simple}.
Note that this criterion is only a heuristic and is suspectible to false positives.

If a solution violates any of our criteria, we recompute the last timestep for the offending cells with a limited finite volume method.
In our case we use the \muscl{}-method (\cref{sec:muscl}).
The recomputation starts by subdividing the infringing cell into $N_s = 2N + 1$ subcells.
This number of subcells has the advantage that we do not lose spatial resolution and that we can use the same timestep size for both cell types.
The reason for this is that the \textsc{cfl}-conditions for both schemes \pcref{eq:cfl-aderdg,eq:cfl-muscl} coincide for our choice of $N_s$.
\begin{figure}[htb]
  \centering
  \includegraphics[scale=0.7]{thesis_aderdg_subcell_grid}
  \caption{\label{fig:limiting-subcells}Conversion from \dg{} degrees of freedoms to finite volume degree of freedoms. Image taken from~\cite{dumbser2018conformal}. }
\end{figure}
\todo{Cite source for subcells~\pcref{fig:limiting-subcells}. See guidebook. NOTE THAT IT IS MODIFIED. Also: Fix size, currently font is a ziny bit too small}

\Cref{fig:limiting-subcells} depicts the mapping from \dg{}-cells to finite volume subcells.
For the sake of brevity, we omit technical details and refer the interestd reader to~\cite{dumbser2016simple}.

\subsection{Adaptive Mesh Refinement}\label{sec:amr}
The goal of adaptive mesh refinement (\amr{}) is to increase the computational efficiency by using a coarse spatial resolution in areas with low information content and a higher resolution in areas of interest.
A simple example for this would be using smaller mesh cells for the simulation of a wave, while using larger cells for areas where the wave does not pass through.
Of course, the difficult part is recognizing interesting areas.
We use a global, gradient based refinement strategy.
We first describe the computation of our indicator variable and then describe a simple way how one can detect outliers.

Let $\bm{f}(\bm{x}): \mathbb{R}^{N_\text{vars}} \to \mathbb{R}$ be a function that maps the discrete solution of a cell to an arbitrary indicator variable.
We assume that $\bm{f}$ is expanded in the space-basis \pcref{eq:cell-approx-space}.
The total variation (\textsc{tv}) of this function is defined by
\begin{equation}
  \label{eq:tv}
  \tv \left[ f(\bm{x}) \right] =
  \Vert
\intdcell{ \vert \gradient{f \left( \bm{x} \right)} \vert }
\Vert_1
\end{equation}
for each cell.
The operator $\Vert \cdot \Vert_1$ denotes the discrete $l_1$ norm in this equation.
This integral can be evaluated efficiently on the reference cell \pcref{eq:space-mapping,eq:cell-approx-space} with Gaussian quadrature \pcref{eq:quadrature}.

\newcommand{\gobs}{\operatorname{G}}
\newcommand{\mean}{\mu}
\newcommand{\std}{\sigma}
\newcommand{\variance}{\std^2}
\newcommand{\gobsCount}{n}
\newcommand{\sampleVariance}{\overline{\variance}}
A simple way of determining outliers is to compute the mean and standard deviation for a set.
Any value that differs by a certain multitude of the standard deviation from the mean can be marked as an outlier.
We thus want to compute the mean and variance over all grid cells.
For computational efficiency and to simplify the implementation, we do this by performing only pairwise merge operations.
The simplest way of computing this is by the textbook definitions of mean $\mean$ and population variance $\variance$ of a random variable $X$
\newcommand{\expectation}{\mathbb{E}}
\begin{align}
  \begin{split}
    \mean [X] &= \expectation \left[ X \right],\\
    \variance [X] &= \expectation \left[ X^2 \right] - \expectation \left[ X \right]^2,
  \end{split}
\end{align}
where $\expectation \left[ X \right]$ denotes the expectation of a random variable $X$.
This naive algorithm is numerically unstable when the variance is orders of magnitudes smaller than the mean, as this can lead to catastrophic cancellations.
We thus compute the mean and variance with the parallel algorithm of~\cite{chan1982updating}.
To do this, we need to store three variables per computation unit:
the mean $\mean$, the current sample variance $\sampleVariance$ and the number of processed elements $n$.
We collect these items in the vector $\gobs = (\mean, \sampleVariance, \gobsCount)$.
We can then merge a pair of observed variables with \cref{alg:merge-variance}.
This algorithm computes an estimate of the population variance using Bessel's correction, i.e.\ it returns the sample variance times $\nicefrac{n}{n-1}$.
In our case, the compute the variance over all grid cells.
This is why we compute the population standard deviation by
\begin{equation}
  \label{eq:sample-std}
 \sigma = \sqrt{ \frac{n-1}{n} \sampleVariance}.
\end{equation}
\begin{algorithm}[tb]
  \begin{algorithmic}
\Function{reduce-variance}{$\gobs_0, \gobs_1$}
\If{$\gobsCount_0 = 0$}
  \State\Return{$\mean_1, \variance_1, \gobsCount_1$}
\EndIf\
\If{$\gobsCount_1 = 0$}
  \State\Return{$\mean_0, \variance_0, \gobsCount_0$}
\EndIf\
  \Let{$\Delta$}{$\mean_1 - \mean_0$}  
  \Let{$\gobsCount_\Sigma$}{$\gobsCount_0 + \gobsCount_1$}
  \Let{$m_a$}{$\variance_0 (\gobsCount_0 - 1)$}
  \Let{$m_b$}{$\variance_1 (\gobsCount_1 - 1)$}
  \Let{$m_\Sigma$}{$\nicefrac{m_a + m_b + (\Delta^2 \gobsCount_0 \gobsCount_1)}{\gobsCount_\Sigma}$}
  \Let{$\mean_\Sigma$}{$\mean_0  \gobsCount_0 + \mean_1 \gobsCount_1$}
  \State\Return{$
    \nicefrac{\mean_\Sigma}{\gobsCount_\Sigma},
    \nicefrac{m_\text{total}}{\gobsCount_\Sigma - 1},
    \gobsCount_\Sigma
    $}
\EndFunction\
  \end{algorithmic}
  \caption{\label{alg:merge-variance}
    Merging two sets of reduced mean and variance~\cite{chan1982updating}}
\end{algorithm}
Chebychev's inequality
\begin{equation}
  \label{eq:chebychev}
  \mathbb{P}(\vert X - \mu \vert \geq c \sigma) \leq \frac{1}{c^2},
\end{equation}
where $\mathbb{P}$ is the probability of an event,
holds for arbitrary distributions with mean $\mu$ and standard deviation $\sigma$~\cite{wasserman2004all}.
This motivates the following refinement criterion
\begin{equation}
  \label{eq:refinement-criterion}
  \operatorname{evaluate-refinement}(\Q, \mu, \sigma) =
  \begin{cases}
    \text{refine} & \text{if } \tv(\Q) \geq \mu + T_\text{refine} \sigma \\
    \text{delete} & \text{if } \tv(\Q) < \mu + T_\text{delete} \sigma \\
    \text{keep} & \text{otherwise}
    \end{cases}
\end{equation}
which refines a cell when its total variations differs by a multiple of the standard deviation.
In this equation, the constants $T_\text{refine}$ and $T_\text{delete}$ can be used to tailor the trade-off between the quality of approximation and computational cost.
They can be chosen freely, as long as $T_\text{refine} < T_\text{delete}$.
Chebychev's inequality~\pcref{eq:chebychev} then guarantees that only a subset of cells is marked for refinement.
Note that this inequality provides only a loose bound.
If one would be willing to assume a distribution of the indicator variable, tighter bounds can be derived~\cite{wasserman2004all}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
